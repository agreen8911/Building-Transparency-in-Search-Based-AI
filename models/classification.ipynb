{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (4.55.3)\n",
      "Requirement already satisfied: filelock in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Classifier (DistilBERT)\n",
    "\n",
    "#### We chose a BERT Classifier for our classification task because we wanted to experiment with a more robust model using a transformer architecture versus a more basic Logistic Regression or SVM model.  Upon researching BERT, we discovered that DistilBERT is a good option compared to traditional BERT by itself.  DistilBERT uses distillation (learns to approximate from a teacher model - BERT) and can really useful producing powerful results with a smaller/faster model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question Number</th>\n",
       "      <th>Question</th>\n",
       "      <th>Notes While Testing</th>\n",
       "      <th>Question Category</th>\n",
       "      <th>LLM</th>\n",
       "      <th>Model Version</th>\n",
       "      <th>Explicit Attribution</th>\n",
       "      <th>Date</th>\n",
       "      <th>Attributed Sources</th>\n",
       "      <th>Number of Sources</th>\n",
       "      <th>Are All Source Links Functional</th>\n",
       "      <th>Ads Included in Response</th>\n",
       "      <th>Multi-modal Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"where does the saying keeping up with the jon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>explicit_attribution</td>\n",
       "      <td>11/2/24</td>\n",
       "      <td>\"www.commonlit.org, www.history.howstuffworks....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>all_links_functional</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>text_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"when did day light savings start in the us\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>History</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>explicit_attribution</td>\n",
       "      <td>11/2/24</td>\n",
       "      <td>\"www.wikipedia.org\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>all_links_functional</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>text_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"what is the doll in the garden about\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>explicit_attribution</td>\n",
       "      <td>11/2/24</td>\n",
       "      <td>\"www.goodreads.com, amazon.com, www.publishers...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>all_links_functional</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>text_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"where is a unitary system of government found\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Politics</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>explicit_attribution</td>\n",
       "      <td>11/2/24</td>\n",
       "      <td>\"www.britannica.com, www.guides.skylinecollege...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>all_links_functional</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>text_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"who dies in season 2 of the originals\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>explicit_attribution</td>\n",
       "      <td>11/2/24</td>\n",
       "      <td>\"www.wikipedia.org\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>all_links_functional</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>text_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question Number                                           Question  \\\n",
       "0                1  \"where does the saying keeping up with the jon...   \n",
       "1                2       \"when did day light savings start in the us\"   \n",
       "2                3             \"what is the doll in the garden about\"   \n",
       "3                4    \"where is a unitary system of government found\"   \n",
       "4                5            \"who dies in season 2 of the originals\"   \n",
       "\n",
       "  Notes While Testing  Question Category     LLM   Model Version  \\\n",
       "0                 NaN  General Knowledge  gemini  gemini_1.5_pro   \n",
       "1                 NaN            History  gemini  gemini_1.5_pro   \n",
       "2                 NaN      Entertainment  gemini  gemini_1.5_pro   \n",
       "3                 NaN           Politics  gemini  gemini_1.5_pro   \n",
       "4                 NaN      Entertainment  gemini  gemini_1.5_pro   \n",
       "\n",
       "   Explicit Attribution     Date  \\\n",
       "0  explicit_attribution  11/2/24   \n",
       "1  explicit_attribution  11/2/24   \n",
       "2  explicit_attribution  11/2/24   \n",
       "3  explicit_attribution  11/2/24   \n",
       "4  explicit_attribution  11/2/24   \n",
       "\n",
       "                                  Attributed Sources  Number of Sources  \\\n",
       "0  \"www.commonlit.org, www.history.howstuffworks....                2.0   \n",
       "1                                \"www.wikipedia.org\"                1.0   \n",
       "2  \"www.goodreads.com, amazon.com, www.publishers...                3.0   \n",
       "3  \"www.britannica.com, www.guides.skylinecollege...                3.0   \n",
       "4                                \"www.wikipedia.org\"                1.0   \n",
       "\n",
       "  Are All Source Links Functional Ads Included in Response  \\\n",
       "0            all_links_functional                   no_ads   \n",
       "1            all_links_functional                   no_ads   \n",
       "2            all_links_functional                   no_ads   \n",
       "3            all_links_functional                   no_ads   \n",
       "4            all_links_functional                   no_ads   \n",
       "\n",
       "  Multi-modal Response  \n",
       "0            text_only  \n",
       "1            text_only  \n",
       "2            text_only  \n",
       "3            text_only  \n",
       "4            text_only  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('llm_source_attribution_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Question Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"where does the saying keeping up with the jon...</td>\n",
       "      <td>General Knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"when did day light savings start in the us\"</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"what is the doll in the garden about\"</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"where is a unitary system of government found\"</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"who dies in season 2 of the originals\"</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Question Category\n",
       "0  \"where does the saying keeping up with the jon...  General Knowledge\n",
       "1       \"when did day light savings start in the us\"            History\n",
       "2             \"what is the doll in the garden about\"      Entertainment\n",
       "3    \"where is a unitary system of government found\"           Politics\n",
       "4            \"who dies in season 2 of the originals\"      Entertainment"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_data = data.drop(columns=['Question Number', 'Notes While Testing', 'LLM', 'Model Version', 'Explicit Attribution', 'Date', 'Attributed Sources', 'Number of Sources', 'Are All Source Links Functional', 'Ads Included in Response', 'Multi-modal Response'])\n",
    "bert_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (2.7.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e19f1fa2e43989c5f31b81180a57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5f4d0637714558b9e836f1171a9d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WeightedLossModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.388322</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.079825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.295133</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.284921</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.331477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.183058</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.554167</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.532895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.127655</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384722</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.047090</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.585833</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.581548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.986618</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.628711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.947728</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.622727</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.630317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.925048</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.595833</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.629167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.889900</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.622727</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.630317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.882387</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.622727</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.630317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adamgreen/.pyenv/versions/3.13.5/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 1.9477283954620361, 'eval_accuracy': 0.7, 'eval_precision': 0.6227272727272727, 'eval_recall': 0.7, 'eval_f1': 0.6303174603174603, 'eval_runtime': 0.1547, 'eval_samples_per_second': 129.293, 'eval_steps_per_second': 32.323, 'epoch': 10.0}\n",
      "Text: What is the capital of France?\n",
      "Predicted Label: Geography\n",
      "Text: Explain the theory of relativity.\n",
      "Predicted Label: Science & Technology\n",
      "Text: Who won the World Cup in 2018?\n",
      "Predicted Label: Geography\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "X_basic = bert_data['Question'][:99]\n",
    "y_basic = bert_data['Question Category'][:99]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_basic)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_basic, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": list(X_train), \"labels\": list(y_train)})\n",
    "test_dataset = Dataset.from_dict({\"text\": list(X_test), \"labels\": list(y_test)})\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "class WeightedLossModel(DistilBertForSequenceClassification):\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=None)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = CrossEntropyLoss(weight=class_weights_tensor)(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "\n",
    "\n",
    "model = WeightedLossModel.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "new_samples = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"Who won the World Cup in 2018?\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(new_samples, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "tokens = {key: val.to(device) for key, val in tokens.items()} \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_classes = probabilities.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "for sample, label in zip(new_samples, predicted_labels):\n",
    "    print(f\"Text: {sample}\")\n",
    "    print(f\"Predicted Label: {label}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing some additional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the longest river in the world?\n",
      "Predicted Label: Geography\n",
      "Question: What is Newton's second law of motion?\n",
      "Predicted Label: Science & Technology\n",
      "Question: Who was the first President of the United States?\n",
      "Predicted Label: History\n",
      "Question: Which country hosted the 2016 Summer Olympics?\n",
      "Predicted Label: Geography\n",
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Predicted Label: History\n",
      "Question: What is the meaning of existentialism?\n",
      "Predicted Label: Science & Technology\n",
      "Question: What is the difference between a bull market and a bear market?\n",
      "Predicted Label: Science & Technology\n",
      "Question: Who directed the movie 'Inception'?\n",
      "Predicted Label: Entertainment\n",
      "Question: How do you make a classic Margherita pizza?\n",
      "Predicted Label: Economics & Commerce\n",
      "Question: What is the primary role of the United Nations?\n",
      "Predicted Label: Politics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_questions = [\n",
    "    \"What is the longest river in the world?\",\n",
    "    \"What is Newton's second law of motion?\",\n",
    "    \"Who was the first President of the United States?\",\n",
    "    \"Which country hosted the 2016 Summer Olympics?\",\n",
    "    \"Who wrote 'Pride and Prejudice'?\",\n",
    "    \"What is the meaning of existentialism?\",\n",
    "    \"What is the difference between a bull market and a bear market?\",\n",
    "    \"Who directed the movie 'Inception'?\",\n",
    "    \"How do you make a classic Margherita pizza?\",\n",
    "    \"What is the primary role of the United Nations?\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sample_questions, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_classes = probabilities.argmax(dim=1).cpu().numpy()  \n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "for question, label in zip(sample_questions, predicted_labels):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose this model because of its contextual understanding of text, pretrained knowledge base, and model customization.  We did test various hyperparameters in our model, including learning rate, batch size, and number of epochs.  Something we saw consistently in all of these tests is the model seemed to plateau around mid way point of the number of epochs, possibly indicating the model was struggling with generalization. Although the classification performance metrics were not very good (slightly above average), we believe with additional hyperparamter tuning and perhaps with a larger dataset, we will see significantly better performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.5",
   "language": "python",
   "name": "py-3.13.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
